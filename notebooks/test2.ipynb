{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Data preprocessing\n",
    "---\n",
    "In this step, we preprocess data from historical data or data generation.\n",
    "\n",
    "### 1. Import the Necessary Packages\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import copy\n",
    "from cycler import cycler\n",
    "\n",
    "# # Set up matplotlib and seaborn styles\n",
    "# plt.style.use('bmh')\n",
    "# mpl.rcParams['axes.prop_cycle'] = cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "\n",
    "np.random.seed(0)  # For reproducibility\n",
    "df_original_data = pd.read_csv('data/data.csv')\n",
    "df_original_data.reset_index(inplace=True, drop=True)\n",
    "df_original_data\n",
    "# Find's boolean mask of non-null values, then sums them by row - only checking kWhs\n",
    "non_null_counts_by_consumer = df_original_data.iloc[:, 2:].notnull().sum(axis=1)\n",
    "\n",
    "# Print first five values to check\n",
    "non_null_counts_by_consumer.head()\n",
    "\n",
    "# Using a distribution plot to visualize non-null counts in data\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(non_null_counts_by_consumer, kde=False, edgecolor='k', linewidth=2)\n",
    "plt.xlabel('Non-Null kWhs')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Non-Null kWhs Before `NaN` Replacement')\n",
    "df_original_data.isnull().sum().sum()\n",
    "# Step 2: Detect and remove rows with more than 600 NaNs\n",
    "threshold = 600  # Set the threshold for NaN values\n",
    "rows_to_drop = df_original_data[df_original_data.isnull().sum(axis=1) > threshold].index  # Get index of rows with more than 600 NaNs\n",
    "\n",
    "# Step 3: Remove these rows from the DataFrame\n",
    "df_original_data = df_original_data.drop(index=rows_to_drop)\n",
    "df_original_data\n",
    "df_original_data.isnull().sum().sum()\n",
    "df_original_data.reset_index(inplace=True, drop=True)\n",
    "# #droping duplicate row\n",
    "# dropIndex = df_data[df_data.duplicated()].index  # duplicates drop\n",
    "# df_data = df_data.drop(dropIndex, axis=0)   # droping duplicate value present wen two row are same\n",
    "# df_info = df_info.drop(dropIndex, axis=0) # droping duplicate index infodata\n",
    "\n",
    "# zeroIndex = df_data[(df_data.sum(axis=1) == 0)].index  # zero rows drop\n",
    "# df_data = df_data.drop(zeroIndex, axis=0)\n",
    "# infoData = df_info.drop(zeroIndex, axis=0)\n",
    "df_info = pd.DataFrame()\n",
    "df_info['CONS_NO'] = df_original_data['CONS_NO']\n",
    "df_info['FLAG'] = df_original_data['FLAG']\n",
    "df_original_data = df_original_data.drop(['FLAG', 'CONS_NO'], axis=1)   #axis 1 column ,axis 0 row\n",
    "#change column name to dates(2014/1/1 to 2014-01-01)\n",
    "df_original_data.columns = pd.to_datetime(df_original_data.columns)  #columns reindexing according to dates\n",
    "\n",
    "#sort data accoding to date( as previusoly column are unsorted)\n",
    "df_original_data = df_original_data.reindex(sorted(df_original_data.columns), axis=1)\n",
    "# Calculating the mean and standard deviation\n",
    "max_by_consumer = df_original_data.max(axis=1)\n",
    "average_by_consumer = df_original_data.mean(axis=1)\n",
    "std_by_consumer = df_original_data.std(axis=1)\n",
    "df_original_data.mean().mean()\n",
    "max_by_consumer.idxmax()\n",
    "df_original_data.loc[5962].idxmax()\n",
    "df_original_data.loc[5962].plot()\n",
    "df_original_data.max().max(), df_original_data.mean().mean()\n",
    "import time\n",
    "\n",
    "# Attempting numpy array-based solution\n",
    "kWhs = df_original_data.values.copy()\n",
    "\n",
    "# Does it have the right shape\n",
    "rows, cols = kWhs.shape\n",
    "\n",
    "# Record time at beginning of loop\n",
    "t_0 = time.time()\n",
    "\n",
    "# For every consumer\n",
    "for i in range(0, rows):\n",
    "  # Print a dot as an update after every 400 consumers\n",
    "  if (i % 400  == 0):\n",
    "    print(\"#\", end=\"\")\n",
    "\n",
    "  # And for every value of that consumer\n",
    "  for j in range(1, cols - 1):\n",
    "    # If the current value is undefined\n",
    "    if (np.isnan(kWhs[i, j])):\n",
    "      if (~np.isnan(kWhs[i, j - 1]) and ~np.isnan(kWhs[i, j + 1])):\n",
    "        kWhs[i, j] = np.mean([kWhs[i, j - 1], kWhs[i, j + 1]])\n",
    "      elif(np.isnan(kWhs[i, j - 1]) or np.isnan(kWhs[i, j + 1])):\n",
    "        kWhs[i, j] = 0.0\n",
    "\n",
    "# Record time at end of loop\n",
    "t_f = time.time()\n",
    "print('\\n', (t_f - t_0)/60)\n",
    "# Replacing kWh values with processed numpy array values in the dataframe\n",
    "df_data_filled = df_original_data.copy(deep=True)\n",
    "df_data_filled.iloc[:, 0:] = kWhs\n",
    "\n",
    "# Remaining `NaN`s are in the first and last columns - replace with 0\n",
    "df_data_filled.fillna(0, inplace=True)\n",
    "df_data_filled\n",
    "df_data_filled.isnull().sum().sum()\n",
    "df_data_filled.max().max(), df_data_filled.mean().mean()\n",
    "# Calculating the mean and standard deviation\n",
    "max_by_consumer = df_data_filled.max(axis=1)\n",
    "average_by_consumer = df_data_filled.mean(axis=1)\n",
    "std_by_consumer = df_data_filled.std(axis=1)\n",
    "df_data_filled.shape[0]\n",
    "df_outliers_handled = df_data_filled.copy(deep=True)\n",
    "for i in range(df_outliers_handled.shape[0]):\n",
    "    mean = df_outliers_handled.loc[i].mean()\n",
    "    sd = df_outliers_handled.loc[i].std()\n",
    "    upper_limit = mean + 2 * sd\n",
    "    lower_limit = mean - 2 * sd\n",
    "    \n",
    "    # Capping the data at the upper and lower sigma limits.\n",
    "    df_outliers_handled.loc[i] = np.where(df_outliers_handled.loc[i] > upper_limit, upper_limit,\n",
    "                                          np.where(df_outliers_handled.loc[i] < lower_limit, lower_limit, df_outliers_handled.loc[i]))\n",
    "\n",
    "# for i in range(df_data_filled.shape[0]):\n",
    "#     mean = df_data_filled.loc[i].mean()\n",
    "#     sd = df_data_filled.loc[i].std()\n",
    "#     arr = df_data_filled.loc[i].values\n",
    "\n",
    "#     arr[arr > (mean + 2 * sd)] = mean + 2 * sd\n",
    "#     df_data_filled.loc[i] = arr\n",
    "\n",
    "# # Calculating the mean and standard deviation\n",
    "# max_by_consumer = df_outliers_handled.max(axis=1)\n",
    "# average_by_consumer = df_outliers_handled.mean(axis=1)\n",
    "# std_by_consumer = df_outliers_handled.std(axis=1)\n",
    "\n",
    "# average_by_consumer\n",
    "# Calculating the mean and standard deviation\n",
    "max_by_consumer = df_outliers_handled.max(axis=1)\n",
    "average_by_consumer = df_outliers_handled.mean(axis=1)\n",
    "std_by_consumer = df_outliers_handled.std(axis=1)\n",
    "average_by_consumer\n",
    "max_by_consumer\n",
    "max_by_consumer.idxmax()\n",
    "df_outliers_handled.loc[5388].plot()\n",
    "df_info.loc[5388]\n",
    "max(df_outliers_handled.loc[5388])\n",
    "df_data_filled.max().max(), df_data_filled.mean().mean()\n",
    "df_outliers_handled.max().max(), df_outliers_handled.mean().mean()\n",
    "df_outliers_handled.isnull().sum().sum()\n",
    "df_final = pd.concat([df_info, df_outliers_handled], axis=1)\n",
    "df_final\n",
    "df_final.to_csv(\"Data/df_final.csv\")\n",
    "# df_data_filled.to_csv(\"Data/new_clean_data.csv\")\n",
    "# df_info.to_csv(\"Data/new_info.csv\")\n",
    "df_final=pd.read_csv('data/df_final.csv')\n",
    "df_final = df_final.drop(['Unnamed: 0'], axis=1)   #axis 1 column ,axis 0 row\n",
    "df_final\n",
    "odf = df_final.copy()\n",
    "# SEPARATING DEPENDENT AND INDEPENDENT VARIABLES\n",
    "x = odf.iloc[:, 2:].values\n",
    "y = odf.iloc[:, 1].values\n",
    "print(\"Shape of x:\", x.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have your data in x and labels in y\n",
    "\n",
    "# Plotting the distribution of classes in y\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "plt.bar(unique_classes, class_counts)\n",
    "plt.xlabel('FLAG')\n",
    "plt.ylabel('COUNT')\n",
    "plt.title('DATASET DISTRIBUTION')\n",
    "plt.show()\n",
    "\n",
    "# Performing PCA to reduce the dimensionality of x for visualization\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "# Plotting the reduced x in 2D\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y, cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('FLAG 1')\n",
    "plt.ylabel('FLAG 0')\n",
    "plt.title('PCA Visualization of Data')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# noramalisation process\n",
    "scale = MinMaxScaler()\n",
    "x_scaled = scale.fit_transform(x)\n",
    "\n",
    "print(\"Shape of x:\", x_scaled.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# intialize pca and logistic regression model\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y == label)[0]\n",
    "\tpyplot.scatter(x_pca[row_ix, 0], x_pca[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have your data in x and labels in y\n",
    "\n",
    "# Plotting the distribution of classes in y\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "plt.bar(unique_classes, class_counts)\n",
    "plt.xlabel('FLAG')\n",
    "plt.ylabel('COUNT')\n",
    "plt.title('DATASET DISTRIBUTION')\n",
    "plt.show()\n",
    "\n",
    "# Performing PCA to reduce the dimensionality of x for visualization\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Plotting the reduced x in 2D\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y, cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('FLAG 1')\n",
    "plt.ylabel('FLAG 0')\n",
    "plt.title('PCA Visualization of Data')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "x_pca\n",
    "# Plotting the reduced x in 2D\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y_resampled, cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('FLAG 1')\n",
    "plt.ylabel('FLAG 0')\n",
    "plt.title('PCA Visualization of Data')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(delta1[:-1], delta1[1:], c=close, s=volume, alpha=0.5)\n",
    "\n",
    "for i in range(0, x_scaled.shape[1]):\n",
    "    ax.scatter(delta1[:-1], delta1[1:], c=close, s=volume, alpha=0.5)\n",
    "\n",
    "unique,count=np.unique(y,return_counts=True)\n",
    "y_dist_val_cnt={k:v for (k,v) in zip(unique,count)}\n",
    "y_dist_val_cnt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "def smote_with_undersampling(X, y, ratio=1.0):\n",
    "    # Count of number of samples in each class\n",
    "    class_counts = Counter(y)\n",
    "\n",
    "    # Determining the class with fewer samples\n",
    "    minority_class = min(class_counts, key=class_counts.get)\n",
    "    majority_class = max(class_counts, key=class_counts.get)\n",
    "\n",
    "    # Creating SMOTE and Undersampler instances with the specified ratio\n",
    "    smote = SMOTE(sampling_strategy=ratio, random_state=42)\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=ratio, random_state=42)\n",
    "\n",
    "    # Applying SMOTE to create synthetic samples for the minority class\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Applying undersampling to reduce the number of samples in the majority class\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "    # Counting the number of samples in each class in the final resampled dataset\n",
    "    final_class_counts = Counter(y_resampled)\n",
    "\n",
    "    return X_resampled, y_resampled, final_class_counts\n",
    "\n",
    "desired_ratio = 1.0\n",
    "x_resampled, y_resampled, final_class_counts = smote_with_undersampling(x_scaled, y, ratio=desired_ratio)\n",
    "# x_resampled, y_resampled, final_class_counts = smote_with_undersampling(x, y, ratio=desired_ratio)\n",
    "\n",
    "# The final_class_counts will now contain equal numbers of samples for both classes\n",
    "print(\"Final class counts:\", final_class_counts)\n",
    "\n",
    "unique,count=np.unique(y,return_counts=True)\n",
    "y_dist_val_cnt={k:v for (k,v) in zip(unique,count)}\n",
    "y_dist_val_cnt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have your data in x and labels in y\n",
    "\n",
    "# Plotting the distribution of classes in y\n",
    "unique_classes, class_counts = np.unique(y_resampled, return_counts=True)\n",
    "plt.bar(unique_classes, class_counts)\n",
    "plt.xlabel('FLAG')\n",
    "plt.ylabel('COUNT')\n",
    "plt.title('DATASET DISTRIBUTION')\n",
    "plt.show()\n",
    "\n",
    "# Performing PCA to reduce the dimensionality of x for visualization\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_resampled)\n",
    "\n",
    "# Plotting the reduced x in 2D\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y_resampled, cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('FLAG 1')\n",
    "plt.ylabel('FLAG 0')\n",
    "plt.title('PCA Visualization of Data')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y == label)[0]\n",
    "\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "X.shape\n",
    "y.shape\n",
    "# rawData = pd.read_csv('data/data.csv')\n",
    "\n",
    "# #data preprocessing \n",
    "# #removing column 1 and 2(making InfoData)\n",
    "# #rawData1_=rawData.iloc[:100,:]\n",
    "# #rawData2_=rawData.iloc[-100:,:]\n",
    "# #rawData=pd.concat([rawData1_, rawData2_], ignore_index=True)\n",
    "# infoData = pd.DataFrame()\n",
    "# infoData['FLAG'] = rawData['FLAG']\n",
    "# infoData['CONS_NO'] = rawData['CONS_NO']\n",
    "# data = rawData.drop(['FLAG', 'CONS_NO'], axis=1)   #axis 1 column ,axis 0 row\n",
    "\n",
    "# #droping duplicate row\n",
    "# dropIndex = data[data.duplicated()].index  # duplicates drop\n",
    "# data = data.drop(dropIndex, axis=0)   #droping duplicate value present wen two row are same\n",
    "# infoData = infoData.drop(dropIndex, axis=0) #droping duplicate index infodata\n",
    "\n",
    "# #removing row with all zero(Nan) value\n",
    "# zeroIndex = data[(data.sum(axis=1) == 0)].index  # zero rows drop\n",
    "# data = data.drop(zeroIndex, axis=0) \n",
    "# infoData = infoData.drop(zeroIndex, axis=0)  \n",
    "\n",
    "# #change column name to dates(2014/1/1 to 2014-01-01)\n",
    "# data.columns = pd.to_datetime(data.columns)  #columns reindexing according to dates\n",
    "\n",
    "# #sort data accoding to date( as previusoly column are unsorted)\n",
    "# data = data.reindex(sorted(data.columns), axis=1)\n",
    "# cols = data.columns\n",
    "\n",
    "# # reindex row name (as some row has been remove till this step due to duplicate or all nan values)\n",
    "# data.reset_index(inplace=True, drop=True)  # index sorting\n",
    "# infoData.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# #filling nan value using neighbouring value (middle missing value replace by average \n",
    "# #and other by maximum 2 distance element)\n",
    "# data = data.interpolate(method='linear', limit=2, limit_direction='both', axis=0).fillna(0) \n",
    "\n",
    "\n",
    "# #removing erronoues value(fixing outliers)\n",
    "# for i in range(data.shape[0]):  # outliers treatment\n",
    "#     m = data.loc[i].mean()\n",
    "#     st = data.loc[i].std()\n",
    "#     data.loc[i] = data.loc[i].mask(data.loc[i] > (m + 3 * st), other=m + 3 * st)\n",
    "\n",
    "data\n",
    "infoData\n",
    "data.to_csv(\"Data/new_clean_data.csv\")\n",
    "infoData.to_csv(\"Data/new_infoData.csv\")\n",
    "data.to_csv(\"../new_clean_data.csv\")\n",
    "infoData.to_csv(\"../new_infoData.csv\")\n",
    "\n",
    "### 2. Dataset\n",
    "rawData = pd.read_csv('data/data.csv')\n",
    "\n",
    "#data preprocessing \n",
    "#removing column 1 and 2(making InfoData)\n",
    "#rawData1_=rawData.iloc[:100,:]\n",
    "#rawData2_=rawData.iloc[-100:,:]\n",
    "#rawData=pd.concat([rawData1_, rawData2_], ignore_index=True)\n",
    "infoData = pd.DataFrame()\n",
    "infoData['FLAG'] = rawData['FLAG']\n",
    "infoData['CONS_NO'] = rawData['CONS_NO']\n",
    "data = rawData.drop(['FLAG', 'CONS_NO'], axis=1)   #axis 1 column ,axis 0 row\n",
    "data\n",
    "#droping duplicate row\n",
    "dropIndex = data[data.duplicated()].index  # duplicates drop\n",
    "data = data.drop(dropIndex, axis=0)   #droping duplicate value present wen two row are same\n",
    "infoData = infoData.drop(dropIndex, axis=0) #droping duplicate index infodata\n",
    "dropIndex\n",
    "data\n",
    "#removing row with all zero(Nan) value\n",
    "zeroIndex = data[(data.sum(axis=1) == 0)].index  # zero rows drop\n",
    "data = data.drop(zeroIndex, axis=0) \n",
    "infoData = infoData.drop(zeroIndex, axis=0)  \n",
    "zeroIndex\n",
    "data\n",
    "#change column name to dates(2014/1/1 to 2014-01-01)\n",
    "data.columns = pd.to_datetime(data.columns)  #columns reindexing according to dates\n",
    "data\n",
    "#sort data accoding to date( as previusoly column are unsorted)\n",
    "data = data.reindex(sorted(data.columns), axis=1)\n",
    "cols = data.columns\n",
    "data\n",
    "# reindex row name (as some row has been remove till this step due to duplicate or all nan values)\n",
    "data.reset_index(inplace=True, drop=True)  # index sorting\n",
    "infoData.reset_index(inplace=True, drop=True)\n",
    "data\n",
    "#filling nan value using neighbouring value (middle missing value replace by average \n",
    "#and other by maximum 2 distance element)\n",
    "data = data.interpolate(method='linear', limit=2, limit_direction='both', axis=0).fillna(0) \n",
    "data\n",
    "#removing erronoues value(fixing outliers)\n",
    "for i in range(data.shape[0]):  # outliers treatment\n",
    "    m = data.loc[i].mean()\n",
    "    st = data.loc[i].std()\n",
    "    data.loc[i] = data.loc[i].mask(data.loc[i] > (m + 3 * st), other=m + 3 * st)\n",
    "\n",
    "data.loc[i][data.loc[i] > (m + 3 * st)]\n",
    "data.shape[0]\n",
    "i = 0\n",
    "m = data.loc[i].mean()\n",
    "st = data.loc[i].std()\n",
    "\n",
    "m, st\n",
    "data.loc[i].mask(data.loc[i] > (m + 3 * st), other=m + 3 * st)\n",
    "other=m + 3 * st\n",
    "other\n",
    "# Step 1: Import the file\n",
    "df = pd.read_csv('data.csv')\n",
    "df.reset_index(inplace=True)\n",
    "df.set_index('CONS_NO', inplace = True)\n",
    "df\n",
    "df.max().max()\n",
    "df.isnull().sum().sum()\n",
    "# Extracting 'Flag' column into a new DataFrame\n",
    "flag_df = df[['FLAG']].copy()\n",
    "df = df.drop(['index', 'FLAG'], axis = 1)\n",
    "df\n",
    "flag_df.head()\n",
    "# Step 2: Detect and remove rows with more than 600 NaNs\n",
    "threshold = 600  # Set the threshold for NaN values\n",
    "rows_to_drop = df[df.isnull().sum(axis=1) > threshold].index  # Get index of rows with more than 600 NaNs\n",
    "\n",
    "# Step 3: Remove these rows from the DataFrame\n",
    "df_cleaned = df.drop(index=rows_to_drop)\n",
    "df_cleaned\n",
    "df_cleaned.isnull().sum().sum()\n",
    "# Method 1\n",
    "# Apply the linear interpolation method\n",
    "# Assuming df is your DataFrame\n",
    "df_interpolated = df_cleaned.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "# Replace NaN values with 0 where interpolation couldn't be applied\n",
    "df_filled = df_cleaned.fillna(0)\n",
    "\n",
    "df_filled\n",
    "\n",
    "df_filled.isnull().sum().sum()\n",
    "# df_filled.to_csv(\"Data/clean_data.csv\")\n",
    "# flag_df.to_csv(\"Data/flag_df.csv\")\n",
    "A = df_filled.values\n",
    "np.any(A < 0)\n",
    "A[A < 0]\n",
    "# Step 1: Import the file\n",
    "df = pd.read_csv('data/clean_data.csv')\n",
    "# df.reset_index(inplace=True)\n",
    "# df.set_index('CONS_NO', inplace = True)\n",
    "df\n",
    "df_flag = pd.read_csv('data/flag_df.csv')\n",
    "df_flag\n",
    "\n",
    "# # Reattempting the interpolation by rows\n",
    "\n",
    "# # Apply custom interpolation row-wise\n",
    "# def interpolate_row(row):\n",
    "#     for i in range(1, len(row) - 1):  # Iterate over each element in the row, excluding first and last\n",
    "#         if pd.isnull(row[i]):  # Check if element is NaN\n",
    "#             if pd.notnull(row[i - 1]) and pd.notnull(row[i + 1]):\n",
    "#                 row[i] = (row[i - 1] + row[i + 1]) / 2  # Set to average of neighbors\n",
    "#             else:\n",
    "#                 row[i] = 0  # Set to 0 if any neighbor is NaN\n",
    "#     return row\n",
    "\n",
    "# # Applying the custom interpolation to each row\n",
    "# df_interpolated = df_cleaned.apply(interpolate_row, axis=1)\n",
    "\n",
    "# df_interpolated.head()\n",
    "\n",
    "# # Define a function for the custom interpolation\n",
    "# def custom_interpolate(series):\n",
    "#     for i in range(1, len(series) - 1):  # Avoid the first and last elements\n",
    "#         if pd.isnull(series[i]):  # Check if NaN\n",
    "#             if pd.notnull(series[i - 1]) and pd.notnull(series[i + 1]):\n",
    "#                 series[i] = (series[i - 1] + series[i + 1]) / 2  # Compute the mean of the neighbors\n",
    "#             else:\n",
    "#                 series[i] = 0  # Set to 0 if neighbors are NaN\n",
    "#     return series\n",
    "\n",
    "# # Apply the custom interpolation function to the DataFrame\n",
    "# df['values'] = custom_interpolate(df['values'])\n",
    "\n",
    "# df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
