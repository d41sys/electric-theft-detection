{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute:  Normal =======================\n",
      "Recall:  0.9938197424892704\n",
      "Precision:  0.986873508353222\n",
      "F1 score:  0.9903344452998032\n",
      "False Negative Rate:  0.6180257510729614\n",
      "Attribute:  Thrief =======================\n",
      "Recall:  0.9868037703513282\n",
      "Precision:  0.9937866758715913\n",
      "F1 score:  0.9902829134061398\n",
      "False Negative Rate:  1.3196229648671807\n",
      "==========================================\n",
      "Macro-Precision:  0.9903300921124066\n",
      "Macro-Recall:  0.9903117564202992\n",
      "Macro-F1 Score:  0.9903086793529715\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix_2 = np.array([[5789,36],\n",
    " [77,5758]])\n",
    "\n",
    "# List of attributes\n",
    "fab_attri = ['Normal', 'max_engine_coolant_temp_attack', 'fuzzing_attack', 'max_speedometer_attack',\n",
    "             'reverse_light_on_attack', 'reverse_light_off_attack', 'correlated_signal_attack']\n",
    "mas_attri = ['Normal', 'max_engine_coolant_temp_attack', 'max_speedometer_attack',\n",
    "             'reverse_light_on_attack', 'reverse_light_off_attack', 'correlated_signal_attack']\n",
    "\n",
    "attri = ['Normal','Thrief']\n",
    "\n",
    "# F1 score calculation for each attribute\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "def cal_cm(confusion_matrix, attributes):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_score_list = []\n",
    "    for i in range(len(attributes)):\n",
    "        TP = confusion_matrix[i, i]\n",
    "        TN = sum(sum(confusion_matrix)) - TP\n",
    "        FP = sum(confusion_matrix[:, i]) - TP\n",
    "        FN = sum(confusion_matrix[i, :]) - TP\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        FNR = (FN / (TP + FN))*100  # False Negative Rate\n",
    "        FPR = (FP / (FP + TN))*100  # False Positive Rate\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_score_list.append(f1_score)\n",
    "        print('Attribute: ', attributes[i], '=======================')\n",
    "        print('Recall: ', recall)\n",
    "        print('Precision: ', precision)\n",
    "        print('F1 score: ', f1_score)\n",
    "        print('False Negative Rate: ', FNR)  # Print FNR\n",
    "    macro_precision = sum(precision_list) / len(precision_list)\n",
    "    macro_recall = sum(recall_list) / len(recall_list)\n",
    "    macro_f1_score = sum(f1_score_list) / len(f1_score_list)\n",
    "    print('==========================================')\n",
    "    print('Macro-Precision: ', macro_precision)\n",
    "    print('Macro-Recall: ', macro_recall)\n",
    "    print('Macro-F1 Score: ', macro_f1_score)\n",
    "    return macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "\n",
    "f1 = cal_cm(confusion_matrix_2, attri)\n",
    "\n",
    "# f1_scores_per_attribute = dict(zip(attributes, f1_scores))\n",
    "# f1_scores_per_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The compared method WDCNNModel is derived from\n",
    "# https://github.com/neuralmind-ai/electricity-theft-detection-with-self-attention/blob/master/CNN_model.py\n",
    "\n",
    "\n",
    "def kernel_fn(kernel, channel_in, channel_out, device):\n",
    "    kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "    kernel = kernel.repeat(channel_out, channel_in, 1, 1).float()\n",
    "    weight = nn.Parameter(data=kernel, requires_grad=False)\n",
    "\n",
    "    return weight.to(device)\n",
    "\n",
    "# random_seed = 123\n",
    "# torch.manual_seed(random_seed)\n",
    "class KernelConv2d(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, stride=1, padding=0, bias=False):\n",
    "        super(KernelConv2d, self).__init__()\n",
    "        self.channel_in = channel_in\n",
    "        self.channel_out = channel_out\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.g1_kernel = [[0.0, -1.0, 0.0],\n",
    "                     [0.0, 2.0, 0.0],\n",
    "                     [0.0, -1.0, 0.0]]\n",
    "        self.g2_kernel = [[0.0, 0.0, 0.0],\n",
    "                     [-1.0, 2.0, -1.0],\n",
    "                     [0.0, 0.0, 0.0]]\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        g1_kernel = kernel_fn(self.g1_kernel, self.channel_in, self.channel_out, x.device) # [channel_out, channel_in, kernel, kernel]\n",
    "        g2_kernel = kernel_fn(self.g2_kernel, self.channel_in, self.channel_out, x.device)\n",
    "        if self.bias:\n",
    "            out_g1 = F.conv2d(x, g1_kernel, stride=self.stride, padding=self.padding, bias=torch.rand(self.channel_out).to(x.device))\n",
    "            out_g2 = F.conv2d(x, g2_kernel, stride=self.stride, padding=self.padding, bias=torch.rand(self.channel_out).to(x.device))\n",
    "        else:\n",
    "            out_g1 = F.conv2d(x, g1_kernel, stride=self.stride, padding=self.padding)\n",
    "            out_g2 = F.conv2d(x, g2_kernel, stride=self.stride, padding=self.padding)\n",
    "        out = torch.tanh(out_g1+out_g2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class WDCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WDCNNModel, self).__init__()\n",
    "\n",
    "        # self.cnn_nc = 16\n",
    "        # self.wide_fc_nc = 50\n",
    "        # self.deep_fc_nc = 60\n",
    "        self.cnn_nc = 60\n",
    "        self.wide_fc_nc = 90\n",
    "        self.deep_fc_nc = 90\n",
    "\n",
    "        self.wide_net = nn.Sequential(OrderedDict([\n",
    "            ('wide_fc', nn.Linear(148 * 7*2, self.wide_fc_nc)),\n",
    "            ('wide_fc_relu', nn.ReLU()),\n",
    "        ]))\n",
    "\n",
    "        self.deep_net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', KernelConv2d(2, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "\n",
    "            ('conv2', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "\n",
    "            ('conv3', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "\n",
    "            ('conv4', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "\n",
    "            ('conv5', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "\n",
    "            ('maxpool', nn.MaxPool2d((1, 7), stride=(1, 7))),\n",
    "        ]))\n",
    "\n",
    "        self.deep_net_fc = nn.Sequential(OrderedDict([\n",
    "            ('deep_fc', nn.Linear(self.cnn_nc * 148, self.deep_fc_nc)),\n",
    "            ('deep_fc_relu', nn.ReLU()),\n",
    "        ]))\n",
    "\n",
    "        self.fusion_fc = nn.Linear(self.wide_fc_nc + self.deep_fc_nc, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # wide&deep model does not use the mask map\n",
    "        # x = x[:, 0:1, :, :]\n",
    "\n",
    "        wide_output = self.wide_net(x.view(x.shape[0], -1))\n",
    "\n",
    "        deep_output = self.deep_net(x)\n",
    "        deep_output = self.deep_net_fc(deep_output.view(deep_output.shape[0], -1))\n",
    "\n",
    "        output = self.fusion_fc(torch.cat((wide_output, deep_output), 1))\n",
    "        if self.training == False:\n",
    "            output = self.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class GroupFC(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, output_nc, group_num=4, view=True):\n",
    "        super(GroupFC, self).__init__()\n",
    "        self.view = view\n",
    "\n",
    "        input_nc = input_shape[0]\n",
    "        h = input_shape[1]\n",
    "        w = input_shape[2]\n",
    "\n",
    "        self.groupFC = nn.Conv2d(input_nc, output_nc, groups=group_num, kernel_size=(h, w), stride=(h, w), padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.groupFC(x)\n",
    "        if self.view:\n",
    "            return out.view([x.shape[0], -1])\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class WDCNNModel_g1g2_mask_699_hyorder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WDCNNModel_g1g2_mask_699_hyorder, self).__init__()\n",
    "\n",
    "        # self.cnn_nc = 16\n",
    "        # self.wide_fc_nc = 50\n",
    "        # self.deep_fc_nc = 60\n",
    "        self.cnn_nc = 60\n",
    "        self.wide_fc_nc = 90\n",
    "        self.deep_fc_nc = 90\n",
    "\n",
    "        self.wide_net = nn.Sequential(OrderedDict([\n",
    "            ('wide_fc', nn.Linear(148 * 7*2, self.wide_fc_nc)),\n",
    "            ('wide_fc_relu', nn.ReLU()),\n",
    "        ]))\n",
    "\n",
    "        self.deep_net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', KernelConv2d(2, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "\n",
    "            ('conv2', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "\n",
    "            ('conv3', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "\n",
    "            ('conv4', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "\n",
    "            ('conv5', KernelConv2d(self.cnn_nc, self.cnn_nc, stride=1, padding=1, bias=True)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "\n",
    "            # ('maxpool', ),\n",
    "        ]))\n",
    "\n",
    "        self.pool = nn.MaxPool2d((1, 7), stride=(1, 7))\n",
    "\n",
    "        self.deep_net_fc = nn.Sequential(OrderedDict([\n",
    "            ('deep_fc', nn.Linear(self.cnn_nc * 148, self.deep_fc_nc)),\n",
    "            ('deep_fc_relu', nn.ReLU()),\n",
    "        ]))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.day_head = 4\n",
    "\n",
    "        w=7\n",
    "        output_dim = 180\n",
    "\n",
    "        self.day_pcc_layer = nn.Sequential(OrderedDict([\n",
    "            ('dense1', GroupFC((self.day_head,\n",
    "                                w,\n",
    "                                w), output_dim,\n",
    "                               group_num=1)),\n",
    "            ('norm1', nn.BatchNorm1d(output_dim)),\n",
    "            ('prelu1', nn.PReLU()),\n",
    "            ('drop1', nn.Dropout(p=0.7))\n",
    "        ]))\n",
    "\n",
    "        self.fusion_fc = nn.Linear(self.wide_fc_nc + self.deep_fc_nc + output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        wide_output = self.wide_net(x.view(x.shape[0], -1))\n",
    "        deep_output = self.deep_net(x)  # b,60,148,7\n",
    "\n",
    "        C = deep_output.shape[1]\n",
    "\n",
    "        day_input = deep_output.permute(0, 3, 1, 2).reshape(N,\n",
    "                                                  W,\n",
    "                                                  self.day_head,\n",
    "                                                  (C * H) // self.day_head).permute(0, 2, 1,\n",
    "                                                                                    3).contiguous()  # N x W x C*H -> N x W x head x (C*H/4) -> N x head x W x (C*H/4)\n",
    "\n",
    "        day_pcc = torch.einsum(\"bnqd,bnkd->bnqk\", day_input, day_input)\n",
    "        second_output = self.day_pcc_layer(day_pcc)  # b,180\n",
    "\n",
    "        deep_output = self.pool(deep_output)\n",
    "        deep_output = self.deep_net_fc(deep_output.view(deep_output.shape[0], -1))\n",
    "        first_output = torch.cat((wide_output, deep_output), 1)  # b,180\n",
    "\n",
    "        output = self.fusion_fc(torch.cat((first_output, second_output), dim=1))  # B,1\n",
    "\n",
    "        if self.training == False:\n",
    "            output = self.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "WDCNNModel                               [8, 1]                    --\n",
      "├─Sequential: 1-1                        [8, 90]                   --\n",
      "│    └─Linear: 2-1                       [8, 90]                   186,570\n",
      "│    └─ReLU: 2-2                         [8, 90]                   --\n",
      "├─Sequential: 1-2                        [8, 60, 148, 1]           --\n",
      "│    └─KernelConv2d: 2-3                 [8, 60, 148, 7]           --\n",
      "│    └─ReLU: 2-4                         [8, 60, 148, 7]           --\n",
      "│    └─KernelConv2d: 2-5                 [8, 60, 148, 7]           --\n",
      "│    └─ReLU: 2-6                         [8, 60, 148, 7]           --\n",
      "│    └─KernelConv2d: 2-7                 [8, 60, 148, 7]           --\n",
      "│    └─ReLU: 2-8                         [8, 60, 148, 7]           --\n",
      "│    └─KernelConv2d: 2-9                 [8, 60, 148, 7]           --\n",
      "│    └─ReLU: 2-10                        [8, 60, 148, 7]           --\n",
      "│    └─KernelConv2d: 2-11                [8, 60, 148, 7]           --\n",
      "│    └─ReLU: 2-12                        [8, 60, 148, 7]           --\n",
      "│    └─MaxPool2d: 2-13                   [8, 60, 148, 1]           --\n",
      "├─Sequential: 1-3                        [8, 90]                   --\n",
      "│    └─Linear: 2-14                      [8, 90]                   799,290\n",
      "│    └─ReLU: 2-15                        [8, 90]                   --\n",
      "├─Linear: 1-4                            [8, 1]                    181\n",
      "├─Sigmoid: 1-5                           [8, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 986,041\n",
      "Trainable params: 986,041\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 7.89\n",
      "==========================================================================================\n",
      "Input size (MB): 0.07\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 3.94\n",
      "Estimated Total Size (MB): 4.02\n",
      "==========================================================================================\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "MACs (G):  7.88688\n",
      "Params (M):  0.986041\n",
      "Running time for a forward pass: 0.0027077198028564453 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import time\n",
    "from thop import profile\n",
    "\n",
    "# Define the model\n",
    "model = WDCNNModel()\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print the summary of the model to get the number of parameters\n",
    "print(summary(model, (8, 2, 148, 7)))\n",
    "\n",
    "# Create a random input tensor with a larger batch size\n",
    "batch_size = 8  # Increase the batch size\n",
    "input_tensor = torch.randn(batch_size, 2, 148, 7).to(device)\n",
    "\n",
    "macs, params = profile(model, [input_tensor])\n",
    "print('MACs (G): ', macs/1000**2)\n",
    "print('Params (M): ', params/1000**2)\n",
    "\n",
    "# Measure the time taken for a forward pass\n",
    "start_time = time.time()\n",
    "output = model(input_tensor)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Running time for a forward pass: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# The compared method HybridAttentionModel = 2*AttnBlock = 2*(LinearAttention+MixedDilationConv) is forked from \n",
    "# https://github.com/neuralmind-ai/electricity-theft-detection-with-self-attention/blob/master/Hybrid_Attn.py\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_heads, out_heads):\n",
    "        super().__init__()\n",
    "        in_features = 7\n",
    "        \n",
    "        in_sz = in_features * in_heads\n",
    "        out_sz = in_features * out_heads\n",
    "        \n",
    "        self.key = nn.Linear(in_sz, out_sz)\n",
    "        self.query = nn.Linear(in_sz, out_sz)\n",
    "        self.value = nn.Linear(in_sz, out_sz)\n",
    "        \n",
    "        self.heads = out_heads\n",
    "        self.in_features = in_features\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        N, L, D = x.shape\n",
    "        x = x.view(N, L, self.heads, -1).contiguous()\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, L, D = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous() # N x L x C x D\n",
    "        # x = x.view(N, L, -1).contiguous() # N x L x C*D\n",
    "        \n",
    "        \n",
    "        # k = self.key(x)  # [32, 148, 16*7=112]\n",
    "        # q = self.query(x)\n",
    "        # v = self.value(x)\n",
    "        x = x.view(N*L, -1).contiguous()  # N x L x C*D\n",
    "        k = self.key(x).view(N, L, -1).contiguous()  # [32, 148, 16*7=112]\n",
    "        q = self.query(x).view(N, L, -1).contiguous()\n",
    "        v = self.value(x).view(N, L, -1).contiguous()\n",
    "        \n",
    "        k = self.split_heads(k)  # [32, 16, 148, 7]\n",
    "        q = self.split_heads(q)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k)  # [32, 16, 148, 148]\n",
    "        scores = scores / math.sqrt(scores.shape[-1])\n",
    "        \n",
    "        weights = F.softmax(scores.float(), dim=-1).type_as(scores) \n",
    "        weights = F.dropout(weights, p=0.5, training=self.training)  # [32, 16, 148, 148]\n",
    "        attention = torch.matmul(weights, v)  # [32, 16, 148, 7]\n",
    "        return attention\n",
    "\n",
    "class MixedDilationConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        dil1 = out_channels // 2\n",
    "        dil2 = out_channels - dil1\n",
    "        self.conv = nn.Conv2d(in_channels, dil1, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, dil2, kernel_size=3, padding=2, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.conv(x)  # [32, 16, 148, 7]\n",
    "        o1 = self.conv1(x)  # [32, 16, 148, 7]\n",
    "        out = torch.cat((o, o1), dim=1)  # [32, 32, 148, 7]\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_dv, in_channels, out_dv, conv_channels):\n",
    "        super().__init__()\n",
    "        self.attn = LinearAttention(in_dv, out_dv)\n",
    "        self.conv = MixedDilationConv(in_channels, conv_channels)\n",
    "        self.context = nn.Conv2d(out_dv+conv_channels, out_dv+conv_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        o = self.attn(x)   # [32, 16, 148, 7]\n",
    "        o1 = self.conv(x)  # [32, 32, 148, 7]\n",
    "        \n",
    "        fo = torch.cat((o, o1), dim=1)\n",
    "        fo = self.context(fo)  # [32, 48, 148, 7]\n",
    "        \n",
    "        return fo\n",
    "\n",
    "class GroupFC(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, output_nc, group_num=4, view=True):\n",
    "        super(GroupFC, self).__init__()\n",
    "        self.view = view\n",
    "\n",
    "        input_nc = input_shape[0]\n",
    "        h = input_shape[1]\n",
    "        w = input_shape[2]\n",
    "\n",
    "        self.groupFC = nn.Conv2d(input_nc, output_nc, groups=group_num, kernel_size=(h, w), stride=(h, w), padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.groupFC(x)\n",
    "        if self.view:\n",
    "            return out.view([x.shape[0], -1])\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class HybridAttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        neurons = 128\n",
    "        drop = 0.5\n",
    "        self.net = nn.Sequential(\n",
    "            AttnBlock(2, 2, 16, 32),\n",
    "            nn.LayerNorm((48, 148, 7)),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            AttnBlock(48, 48, 16, 32),\n",
    "            nn.LayerNorm((48, 148, 7)), \n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(48 * 1036, neurons * 8),\n",
    "            nn.BatchNorm1d(neurons * 8),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(neurons * 8, 1),\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        #x = x.view(N, C, 147, -1)\n",
    "        o = self.net(x)  # [32, 48, 148, 7]\n",
    "        o = self.classifier(o.view(N, -1))  # [32, 1]\n",
    "\n",
    "        if self.training == False:\n",
    "           o = self.sigmoid(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "class HybridAttentionModel_2nd(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        neurons = 128\n",
    "        drop = 0.5\n",
    "        self.net1 = nn.Sequential(\n",
    "            AttnBlock(2, 2, 16, 32),\n",
    "            nn.LayerNorm((48, 148, 7)),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "        self.net2 = nn.Sequential(\n",
    "            AttnBlock(48, 48, 16, 32),\n",
    "            nn.LayerNorm((48, 148, 7)),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(48 * 1036 * 2, neurons * 8),\n",
    "            nn.BatchNorm1d(neurons * 8),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(neurons * 8, 1),\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.day_head = 4\n",
    "\n",
    "        w = 7\n",
    "        output_dim = 48 * 1036\n",
    "\n",
    "        self.day_pcc_layer = nn.Sequential(OrderedDict([\n",
    "            ('dense1', GroupFC((self.day_head,\n",
    "                                w,\n",
    "                                w), output_dim,\n",
    "                               group_num=1)),\n",
    "            ('norm1', nn.BatchNorm1d(output_dim)),\n",
    "            ('prelu1', nn.PReLU()),\n",
    "            ('drop1', nn.Dropout(p=0.7))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N = x.shape[0]\n",
    "        # x = x.view(N, C, 147, -1)\n",
    "        o = self.net1(x)  # [32, 48, 148, 7]\n",
    "\n",
    "        N, C, H, W = o.shape\n",
    "        day_input = o.permute(0, 3, 1, 2).reshape(N,  W, self.day_head, (C * H) // self.day_head).permute(0, 2, 1,\n",
    "                                                                                              3).contiguous()\n",
    "        # N x W x C*H -> N x W x head x (C*H/4) -> N x head x W x (C*H/4)\n",
    "\n",
    "        day_pcc = torch.einsum(\"bnqd,bnkd->bnqk\", day_input, day_input)  # b, 4,7,7\n",
    "        second_output = self.day_pcc_layer(day_pcc)  # b,c\n",
    "\n",
    "        o = self.net2(o)\n",
    "        o = self.classifier(torch.cat([o.view(N, -1), second_output], dim=1))  # [32, 1]\n",
    "\n",
    "        if self.training == False:\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "class HybridAttentionModel_hyorder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        neurons = 128\n",
    "        drop = 0.5\n",
    "        self.net = nn.Sequential(\n",
    "            AttnBlock(2, 2, 16, 32),\n",
    "            nn.LayerNorm((48, 148, 7)),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            AttnBlock(48, 48, 16, 32),\n",
    "            nn.LayerNorm((48, 148, 7)),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "        # self.net_fc = nn.Sequential(\n",
    "        #     nn.Linear(48 * 1036, neurons * 8),\n",
    "        #     nn.BatchNorm1d(neurons * 8),\n",
    "        #     nn.PReLU(),\n",
    "        #     nn.Dropout(0.6)\n",
    "        # )\n",
    "\n",
    "        output_dim = neurons * 8\n",
    "\n",
    "        self.net_groupfc = nn.Sequential(OrderedDict([\n",
    "            ('dense1', GroupFC((48,\n",
    "                                148,\n",
    "                                7), output_dim,\n",
    "                               group_num=1)),\n",
    "            ('norm1', nn.BatchNorm1d(output_dim)),\n",
    "            ('prelu1', nn.PReLU()),\n",
    "            ('drop1', nn.Dropout(p=0.7))\n",
    "        ]))\n",
    "\n",
    "        self.classifier = nn.Linear(output_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.day_head = 4\n",
    "\n",
    "        w = 7\n",
    "\n",
    "        self.day_pcc_layer = nn.Sequential(OrderedDict([\n",
    "            ('dense1', GroupFC((self.day_head,\n",
    "                                w,\n",
    "                                w), output_dim,\n",
    "                               group_num=1)),\n",
    "            ('norm1', nn.BatchNorm1d(output_dim)),\n",
    "            ('prelu1', nn.PReLU()),\n",
    "            ('drop1', nn.Dropout(p=0.7))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N = x.shape[0]\n",
    "        # x = x.view(N, C, 147, -1)\n",
    "        o = self.net(x)  # [32, 48, 148, 7]\n",
    "\n",
    "        N, C, H, W = o.shape\n",
    "        day_input = o.permute(0, 3, 1, 2).reshape(N,  W, self.day_head, (C * H) // self.day_head).permute(0, 2, 1,\n",
    "                                                                                              3).contiguous()\n",
    "        # N x W x C*H -> N x W x head x (C*H/4) -> N x head x W x (C*H/4)\n",
    "\n",
    "        day_pcc = torch.einsum(\"bnqd,bnkd->bnqk\", day_input, day_input)  # b, 4,7,7\n",
    "        second_output = self.day_pcc_layer(day_pcc)  # b,c\n",
    "\n",
    "        # o = self.net_fc(o.view(N, -1))\n",
    "        o = self.net_groupfc(o)  # b, 1024\n",
    "        o = self.classifier(torch.cat([o, second_output], dim=1))  # [32, 1]\n",
    "\n",
    "        if self.training == False:\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 112]           1,680\n",
      "            Linear-2                  [-1, 112]           1,680\n",
      "            Linear-3                  [-1, 112]           1,680\n",
      "   LinearAttention-4           [-1, 16, 148, 7]               0\n",
      "            Conv2d-5           [-1, 16, 148, 7]             304\n",
      "            Conv2d-6           [-1, 16, 148, 7]             304\n",
      " MixedDilationConv-7           [-1, 32, 148, 7]               0\n",
      "            Conv2d-8           [-1, 48, 148, 7]           2,352\n",
      "         AttnBlock-9           [-1, 48, 148, 7]               0\n",
      "        LayerNorm-10           [-1, 48, 148, 7]          99,456\n",
      "            PReLU-11           [-1, 48, 148, 7]               1\n",
      "          Dropout-12           [-1, 48, 148, 7]               0\n",
      "           Conv2d-13          [-1, 49728, 1, 1]       9,796,416\n",
      "          GroupFC-14                [-1, 49728]               0\n",
      "      BatchNorm1d-15                [-1, 49728]          99,456\n",
      "            PReLU-16                [-1, 49728]               1\n",
      "          Dropout-17                [-1, 49728]               0\n",
      "           Linear-18                  [-1, 112]          37,744\n",
      "           Linear-19                  [-1, 112]          37,744\n",
      "           Linear-20                  [-1, 112]          37,744\n",
      "  LinearAttention-21           [-1, 16, 148, 7]               0\n",
      "           Conv2d-22           [-1, 16, 148, 7]           6,928\n",
      "           Conv2d-23           [-1, 16, 148, 7]           6,928\n",
      "MixedDilationConv-24           [-1, 32, 148, 7]               0\n",
      "           Conv2d-25           [-1, 48, 148, 7]           2,352\n",
      "        AttnBlock-26           [-1, 48, 148, 7]               0\n",
      "        LayerNorm-27           [-1, 48, 148, 7]          99,456\n",
      "            PReLU-28           [-1, 48, 148, 7]               1\n",
      "          Dropout-29           [-1, 48, 148, 7]               0\n",
      "           Linear-30                 [-1, 1024]     101,843,968\n",
      "      BatchNorm1d-31                 [-1, 1024]           2,048\n",
      "            PReLU-32                 [-1, 1024]               1\n",
      "          Dropout-33                 [-1, 1024]               0\n",
      "           Linear-34                    [-1, 1]           1,025\n",
      "================================================================\n",
      "Total params: 112,079,269\n",
      "Trainable params: 112,079,269\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.99\n",
      "Params size (MB): 427.55\n",
      "Estimated Total Size (MB): 434.55\n",
      "----------------------------------------------------------------\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "MACs (G):  1195.510272\n",
      "Params (M):  112.079269\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = HybridAttentionModel_2nd()\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print the summary of the model to get the number of parameters\n",
    "summary(model, (2, 148, 7))\n",
    "\n",
    "# Create a random input tensor with a larger batch size\n",
    "batch_size = 8  # Increase the batch size\n",
    "input_tensor = torch.randn(batch_size, 2, 148, 7).to(device)\n",
    "\n",
    "macs, params = profile(model, [input_tensor])\n",
    "print('MACs (G): ', macs/1000**2)\n",
    "print('Params (M): ', params/1000**2)\n",
    "\n",
    "# # Measure the time taken for a forward pass\n",
    "# start_time = time.time()\n",
    "# output = model(input_tensor)\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(f\"Running time for a forward pass: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "# The code of TemporalConvNet is forked from\n",
    "# https://github.com/locuslab/TCN\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):  # a resblock in PFSC，three TCN layers\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv3 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp3 = Chomp1d(padding)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2,\n",
    "                                 self.conv3, self.chomp3, self.relu3, self.dropout3)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):  # 对应论文中的三个TCN Block\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class PFSC(nn.Module):\n",
    "    def __init__(self, input_size=1, num_channels=[64, 64, 64], kernel_size=2, dropout=0.45):\n",
    "        super(PFSC, self).__init__()\n",
    "\n",
    "        self.tcn_block1 = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)  # a tcn block with 2 res blocks\n",
    "        self.tcn_block2 = TemporalConvNet(num_channels[0], num_channels, kernel_size, dropout=dropout)\n",
    "        self.tcn_block3 = TemporalConvNet(num_channels[1], num_channels, kernel_size, dropout=dropout)\n",
    "\n",
    "        self.dense = nn.Linear(num_channels[-1]*6, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, elec):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        N, C, L = elec.shape\n",
    "\n",
    "        output1 = self.tcn_block1(elec)\n",
    "        output2 = self.tcn_block2(output1)\n",
    "        output3 = self.tcn_block2(output2)\n",
    "\n",
    "        output = output3.view(N, -1).contiguous()\n",
    "\n",
    "        pred = self.dense(output).double()\n",
    "\n",
    "        if self.training == False:\n",
    "            pred = self.sigmoid(pred)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1                [-1, 64, 7]             192\n",
      "            Conv1d-2                [-1, 64, 7]             192\n",
      "           Chomp1d-3                [-1, 64, 6]               0\n",
      "           Chomp1d-4                [-1, 64, 6]               0\n",
      "              ReLU-5                [-1, 64, 6]               0\n",
      "              ReLU-6                [-1, 64, 6]               0\n",
      "           Dropout-7                [-1, 64, 6]               0\n",
      "           Dropout-8                [-1, 64, 6]               0\n",
      "            Conv1d-9                [-1, 64, 7]           8,256\n",
      "           Conv1d-10                [-1, 64, 7]           8,256\n",
      "          Chomp1d-11                [-1, 64, 6]               0\n",
      "          Chomp1d-12                [-1, 64, 6]               0\n",
      "             ReLU-13                [-1, 64, 6]               0\n",
      "             ReLU-14                [-1, 64, 6]               0\n",
      "          Dropout-15                [-1, 64, 6]               0\n",
      "          Dropout-16                [-1, 64, 6]               0\n",
      "           Conv1d-17                [-1, 64, 7]           8,256\n",
      "           Conv1d-18                [-1, 64, 7]           8,256\n",
      "          Chomp1d-19                [-1, 64, 6]               0\n",
      "          Chomp1d-20                [-1, 64, 6]               0\n",
      "             ReLU-21                [-1, 64, 6]               0\n",
      "             ReLU-22                [-1, 64, 6]               0\n",
      "          Dropout-23                [-1, 64, 6]               0\n",
      "          Dropout-24                [-1, 64, 6]               0\n",
      "           Conv1d-25                [-1, 64, 6]             128\n",
      "             ReLU-26                [-1, 64, 6]               0\n",
      "    TemporalBlock-27                [-1, 64, 6]               0\n",
      "           Conv1d-28                [-1, 64, 8]           8,256\n",
      "           Conv1d-29                [-1, 64, 8]           8,256\n",
      "          Chomp1d-30                [-1, 64, 6]               0\n",
      "          Chomp1d-31                [-1, 64, 6]               0\n",
      "             ReLU-32                [-1, 64, 6]               0\n",
      "             ReLU-33                [-1, 64, 6]               0\n",
      "          Dropout-34                [-1, 64, 6]               0\n",
      "          Dropout-35                [-1, 64, 6]               0\n",
      "           Conv1d-36                [-1, 64, 8]           8,256\n",
      "           Conv1d-37                [-1, 64, 8]           8,256\n",
      "          Chomp1d-38                [-1, 64, 6]               0\n",
      "          Chomp1d-39                [-1, 64, 6]               0\n",
      "             ReLU-40                [-1, 64, 6]               0\n",
      "             ReLU-41                [-1, 64, 6]               0\n",
      "          Dropout-42                [-1, 64, 6]               0\n",
      "          Dropout-43                [-1, 64, 6]               0\n",
      "           Conv1d-44                [-1, 64, 8]           8,256\n",
      "           Conv1d-45                [-1, 64, 8]           8,256\n",
      "          Chomp1d-46                [-1, 64, 6]               0\n",
      "          Chomp1d-47                [-1, 64, 6]               0\n",
      "             ReLU-48                [-1, 64, 6]               0\n",
      "             ReLU-49                [-1, 64, 6]               0\n",
      "          Dropout-50                [-1, 64, 6]               0\n",
      "          Dropout-51                [-1, 64, 6]               0\n",
      "             ReLU-52                [-1, 64, 6]               0\n",
      "    TemporalBlock-53                [-1, 64, 6]               0\n",
      "           Conv1d-54               [-1, 64, 10]           8,256\n",
      "           Conv1d-55               [-1, 64, 10]           8,256\n",
      "          Chomp1d-56                [-1, 64, 6]               0\n",
      "          Chomp1d-57                [-1, 64, 6]               0\n",
      "             ReLU-58                [-1, 64, 6]               0\n",
      "             ReLU-59                [-1, 64, 6]               0\n",
      "          Dropout-60                [-1, 64, 6]               0\n",
      "          Dropout-61                [-1, 64, 6]               0\n",
      "           Conv1d-62               [-1, 64, 10]           8,256\n",
      "           Conv1d-63               [-1, 64, 10]           8,256\n",
      "          Chomp1d-64                [-1, 64, 6]               0\n",
      "          Chomp1d-65                [-1, 64, 6]               0\n",
      "             ReLU-66                [-1, 64, 6]               0\n",
      "             ReLU-67                [-1, 64, 6]               0\n",
      "          Dropout-68                [-1, 64, 6]               0\n",
      "          Dropout-69                [-1, 64, 6]               0\n",
      "           Conv1d-70               [-1, 64, 10]           8,256\n",
      "           Conv1d-71               [-1, 64, 10]           8,256\n",
      "          Chomp1d-72                [-1, 64, 6]               0\n",
      "          Chomp1d-73                [-1, 64, 6]               0\n",
      "             ReLU-74                [-1, 64, 6]               0\n",
      "             ReLU-75                [-1, 64, 6]               0\n",
      "          Dropout-76                [-1, 64, 6]               0\n",
      "          Dropout-77                [-1, 64, 6]               0\n",
      "             ReLU-78                [-1, 64, 6]               0\n",
      "    TemporalBlock-79                [-1, 64, 6]               0\n",
      "  TemporalConvNet-80                [-1, 64, 6]               0\n",
      "           Conv1d-81                [-1, 64, 7]           8,256\n",
      "           Conv1d-82                [-1, 64, 7]           8,256\n",
      "          Chomp1d-83                [-1, 64, 6]               0\n",
      "          Chomp1d-84                [-1, 64, 6]               0\n",
      "             ReLU-85                [-1, 64, 6]               0\n",
      "             ReLU-86                [-1, 64, 6]               0\n",
      "          Dropout-87                [-1, 64, 6]               0\n",
      "          Dropout-88                [-1, 64, 6]               0\n",
      "           Conv1d-89                [-1, 64, 7]           8,256\n",
      "           Conv1d-90                [-1, 64, 7]           8,256\n",
      "          Chomp1d-91                [-1, 64, 6]               0\n",
      "          Chomp1d-92                [-1, 64, 6]               0\n",
      "             ReLU-93                [-1, 64, 6]               0\n",
      "             ReLU-94                [-1, 64, 6]               0\n",
      "          Dropout-95                [-1, 64, 6]               0\n",
      "          Dropout-96                [-1, 64, 6]               0\n",
      "           Conv1d-97                [-1, 64, 7]           8,256\n",
      "           Conv1d-98                [-1, 64, 7]           8,256\n",
      "          Chomp1d-99                [-1, 64, 6]               0\n",
      "         Chomp1d-100                [-1, 64, 6]               0\n",
      "            ReLU-101                [-1, 64, 6]               0\n",
      "            ReLU-102                [-1, 64, 6]               0\n",
      "         Dropout-103                [-1, 64, 6]               0\n",
      "         Dropout-104                [-1, 64, 6]               0\n",
      "            ReLU-105                [-1, 64, 6]               0\n",
      "   TemporalBlock-106                [-1, 64, 6]               0\n",
      "          Conv1d-107                [-1, 64, 8]           8,256\n",
      "          Conv1d-108                [-1, 64, 8]           8,256\n",
      "         Chomp1d-109                [-1, 64, 6]               0\n",
      "         Chomp1d-110                [-1, 64, 6]               0\n",
      "            ReLU-111                [-1, 64, 6]               0\n",
      "            ReLU-112                [-1, 64, 6]               0\n",
      "         Dropout-113                [-1, 64, 6]               0\n",
      "         Dropout-114                [-1, 64, 6]               0\n",
      "          Conv1d-115                [-1, 64, 8]           8,256\n",
      "          Conv1d-116                [-1, 64, 8]           8,256\n",
      "         Chomp1d-117                [-1, 64, 6]               0\n",
      "         Chomp1d-118                [-1, 64, 6]               0\n",
      "            ReLU-119                [-1, 64, 6]               0\n",
      "            ReLU-120                [-1, 64, 6]               0\n",
      "         Dropout-121                [-1, 64, 6]               0\n",
      "         Dropout-122                [-1, 64, 6]               0\n",
      "          Conv1d-123                [-1, 64, 8]           8,256\n",
      "          Conv1d-124                [-1, 64, 8]           8,256\n",
      "         Chomp1d-125                [-1, 64, 6]               0\n",
      "         Chomp1d-126                [-1, 64, 6]               0\n",
      "            ReLU-127                [-1, 64, 6]               0\n",
      "            ReLU-128                [-1, 64, 6]               0\n",
      "         Dropout-129                [-1, 64, 6]               0\n",
      "         Dropout-130                [-1, 64, 6]               0\n",
      "            ReLU-131                [-1, 64, 6]               0\n",
      "   TemporalBlock-132                [-1, 64, 6]               0\n",
      "          Conv1d-133               [-1, 64, 10]           8,256\n",
      "          Conv1d-134               [-1, 64, 10]           8,256\n",
      "         Chomp1d-135                [-1, 64, 6]               0\n",
      "         Chomp1d-136                [-1, 64, 6]               0\n",
      "            ReLU-137                [-1, 64, 6]               0\n",
      "            ReLU-138                [-1, 64, 6]               0\n",
      "         Dropout-139                [-1, 64, 6]               0\n",
      "         Dropout-140                [-1, 64, 6]               0\n",
      "          Conv1d-141               [-1, 64, 10]           8,256\n",
      "          Conv1d-142               [-1, 64, 10]           8,256\n",
      "         Chomp1d-143                [-1, 64, 6]               0\n",
      "         Chomp1d-144                [-1, 64, 6]               0\n",
      "            ReLU-145                [-1, 64, 6]               0\n",
      "            ReLU-146                [-1, 64, 6]               0\n",
      "         Dropout-147                [-1, 64, 6]               0\n",
      "         Dropout-148                [-1, 64, 6]               0\n",
      "          Conv1d-149               [-1, 64, 10]           8,256\n",
      "          Conv1d-150               [-1, 64, 10]           8,256\n",
      "         Chomp1d-151                [-1, 64, 6]               0\n",
      "         Chomp1d-152                [-1, 64, 6]               0\n",
      "            ReLU-153                [-1, 64, 6]               0\n",
      "            ReLU-154                [-1, 64, 6]               0\n",
      "         Dropout-155                [-1, 64, 6]               0\n",
      "         Dropout-156                [-1, 64, 6]               0\n",
      "            ReLU-157                [-1, 64, 6]               0\n",
      "   TemporalBlock-158                [-1, 64, 6]               0\n",
      " TemporalConvNet-159                [-1, 64, 6]               0\n",
      "          Conv1d-160                [-1, 64, 7]           8,256\n",
      "          Conv1d-161                [-1, 64, 7]           8,256\n",
      "         Chomp1d-162                [-1, 64, 6]               0\n",
      "         Chomp1d-163                [-1, 64, 6]               0\n",
      "            ReLU-164                [-1, 64, 6]               0\n",
      "            ReLU-165                [-1, 64, 6]               0\n",
      "         Dropout-166                [-1, 64, 6]               0\n",
      "         Dropout-167                [-1, 64, 6]               0\n",
      "          Conv1d-168                [-1, 64, 7]           8,256\n",
      "          Conv1d-169                [-1, 64, 7]           8,256\n",
      "         Chomp1d-170                [-1, 64, 6]               0\n",
      "         Chomp1d-171                [-1, 64, 6]               0\n",
      "            ReLU-172                [-1, 64, 6]               0\n",
      "            ReLU-173                [-1, 64, 6]               0\n",
      "         Dropout-174                [-1, 64, 6]               0\n",
      "         Dropout-175                [-1, 64, 6]               0\n",
      "          Conv1d-176                [-1, 64, 7]           8,256\n",
      "          Conv1d-177                [-1, 64, 7]           8,256\n",
      "         Chomp1d-178                [-1, 64, 6]               0\n",
      "         Chomp1d-179                [-1, 64, 6]               0\n",
      "            ReLU-180                [-1, 64, 6]               0\n",
      "            ReLU-181                [-1, 64, 6]               0\n",
      "         Dropout-182                [-1, 64, 6]               0\n",
      "         Dropout-183                [-1, 64, 6]               0\n",
      "            ReLU-184                [-1, 64, 6]               0\n",
      "   TemporalBlock-185                [-1, 64, 6]               0\n",
      "          Conv1d-186                [-1, 64, 8]           8,256\n",
      "          Conv1d-187                [-1, 64, 8]           8,256\n",
      "         Chomp1d-188                [-1, 64, 6]               0\n",
      "         Chomp1d-189                [-1, 64, 6]               0\n",
      "            ReLU-190                [-1, 64, 6]               0\n",
      "            ReLU-191                [-1, 64, 6]               0\n",
      "         Dropout-192                [-1, 64, 6]               0\n",
      "         Dropout-193                [-1, 64, 6]               0\n",
      "          Conv1d-194                [-1, 64, 8]           8,256\n",
      "          Conv1d-195                [-1, 64, 8]           8,256\n",
      "         Chomp1d-196                [-1, 64, 6]               0\n",
      "         Chomp1d-197                [-1, 64, 6]               0\n",
      "            ReLU-198                [-1, 64, 6]               0\n",
      "            ReLU-199                [-1, 64, 6]               0\n",
      "         Dropout-200                [-1, 64, 6]               0\n",
      "         Dropout-201                [-1, 64, 6]               0\n",
      "          Conv1d-202                [-1, 64, 8]           8,256\n",
      "          Conv1d-203                [-1, 64, 8]           8,256\n",
      "         Chomp1d-204                [-1, 64, 6]               0\n",
      "         Chomp1d-205                [-1, 64, 6]               0\n",
      "            ReLU-206                [-1, 64, 6]               0\n",
      "            ReLU-207                [-1, 64, 6]               0\n",
      "         Dropout-208                [-1, 64, 6]               0\n",
      "         Dropout-209                [-1, 64, 6]               0\n",
      "            ReLU-210                [-1, 64, 6]               0\n",
      "   TemporalBlock-211                [-1, 64, 6]               0\n",
      "          Conv1d-212               [-1, 64, 10]           8,256\n",
      "          Conv1d-213               [-1, 64, 10]           8,256\n",
      "         Chomp1d-214                [-1, 64, 6]               0\n",
      "         Chomp1d-215                [-1, 64, 6]               0\n",
      "            ReLU-216                [-1, 64, 6]               0\n",
      "            ReLU-217                [-1, 64, 6]               0\n",
      "         Dropout-218                [-1, 64, 6]               0\n",
      "         Dropout-219                [-1, 64, 6]               0\n",
      "          Conv1d-220               [-1, 64, 10]           8,256\n",
      "          Conv1d-221               [-1, 64, 10]           8,256\n",
      "         Chomp1d-222                [-1, 64, 6]               0\n",
      "         Chomp1d-223                [-1, 64, 6]               0\n",
      "            ReLU-224                [-1, 64, 6]               0\n",
      "            ReLU-225                [-1, 64, 6]               0\n",
      "         Dropout-226                [-1, 64, 6]               0\n",
      "         Dropout-227                [-1, 64, 6]               0\n",
      "          Conv1d-228               [-1, 64, 10]           8,256\n",
      "          Conv1d-229               [-1, 64, 10]           8,256\n",
      "         Chomp1d-230                [-1, 64, 6]               0\n",
      "         Chomp1d-231                [-1, 64, 6]               0\n",
      "            ReLU-232                [-1, 64, 6]               0\n",
      "            ReLU-233                [-1, 64, 6]               0\n",
      "         Dropout-234                [-1, 64, 6]               0\n",
      "         Dropout-235                [-1, 64, 6]               0\n",
      "            ReLU-236                [-1, 64, 6]               0\n",
      "   TemporalBlock-237                [-1, 64, 6]               0\n",
      " TemporalConvNet-238                [-1, 64, 6]               0\n",
      "          Linear-239                    [-1, 1]             385\n",
      "================================================================\n",
      "Total params: 430,209\n",
      "Trainable params: 430,209\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.76\n",
      "Params size (MB): 1.64\n",
      "Estimated Total Size (MB): 2.40\n",
      "----------------------------------------------------------------\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "MACs (G):  7.147776\n",
      "Params (M):  0.283905\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = PFSC(input_size=1, num_channels=[64, 64, 64], kernel_size=2, dropout=0.45)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print the summary of the model to get the number of parameters\n",
    "summary(model, (1, 6))  # Assuming the input length (L) is 1000\n",
    "\n",
    "# Create a random input tensor with the same shape as the model expects\n",
    "input_tensor = torch.randn(1, 1, 6).to(device)  # Assuming batch size 1 and input length 1000\n",
    "\n",
    "macs, params = profile(model, [input_tensor])\n",
    "print('MACs (G): ', macs/1000**2)\n",
    "print('Params (M): ', params/1000**2)\n",
    "\n",
    "# # Measure the time taken for a forward pass\n",
    "# start_time = time.time()\n",
    "# output = model(input_tensor)\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(f\"Running time for a forward pass: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if getattr(torch, 'has_mps', False) else 'cpu')\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'ETD'\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if getattr(torch, 'has_mps', False) else 'cpu')\n",
    "\n",
    "        self.dout_mess = 28 # 4 weeks\n",
    "        self.d_model = self.dout_mess\n",
    "        self.nhead = 7  # ori: 5\n",
    "\n",
    "        self.pad_size = 37  \n",
    "        self.window_size = 37 \n",
    "        self.max_time_position = 10000\n",
    "        self.num_layers = 6\n",
    "        self.gran = 1e-7  # ori: 1e-6\n",
    "        self.log_e = 2\n",
    "\n",
    "        self.classes_num = 2\n",
    "        # self.model_path = 'model/' + self.model_name + '/' + self.model_name + '_model_' + str(self.start_epoch) + '.pth'\n",
    "\n",
    "class ConvAutoencoder1D(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ConvAutoencoder1D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n",
    "            # Original: input_dim -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            # Original: 32 -> 64\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(128*28, output_dim)\n",
    "        # Original: 64*28\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        self.pad_size = config.pad_size\n",
    "\n",
    "        self.cae = ConvAutoencoder1D(1, config.dout_mess).to(device)\n",
    "        self.dout_mess = config.dout_mess\n",
    "        self.mode = 'cae'\n",
    "        \n",
    "        self.position_embedding = PositionalEncoding(config.d_model, dropout=0.0, max_len=config.max_time_position).to(device)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=config.d_model, nhead=config.nhead).to(device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=config.num_layers).to(device)\n",
    "        self.fc = nn.Linear(config.d_model, config.classes_num).to(device)\n",
    "\n",
    "    def forward(self, data, mask):\n",
    "        x = data\n",
    "        \n",
    "        if self.mode == 'cae':\n",
    "            # Conv Autoencoder 1D =================================================   \n",
    "            cae_out = torch.empty((x.shape[0], self.dout_mess, 0)).to(device)\n",
    "            for i in range(self.pad_size):\n",
    "                tmp = self.cae(x[:, i:i+1, :]).unsqueeze(2)\n",
    "                cae_out = torch.concat((cae_out, tmp), dim=2)\n",
    "            \n",
    "            x = cae_out.permute(2, 0, 1)\n",
    "        else:\n",
    "            x = x.permute(1, 0, 2)\n",
    "            \n",
    "        # x = x.permute(1, 0, 2)\n",
    "        \n",
    "        out = self.position_embedding(x)\n",
    "        out2 = self.transformer_encoder(out, src_key_padding_mask=mask)\n",
    "        out = out2.permute(1, 0, 2)\n",
    "        out = torch.sum(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "TransformerPredictor                               [10, 2]                   120,124\n",
      "├─ConvAutoencoder1D: 1-1                           [10, 28]                  --\n",
      "│    └─Sequential: 2-1                             [10, 128, 28]             --\n",
      "│    │    └─Conv1d: 3-1                            [10, 64, 28]              256\n",
      "│    │    └─ReLU: 3-2                              [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-3                            [10, 128, 28]             24,704\n",
      "│    │    └─ReLU: 3-4                              [10, 128, 28]             --\n",
      "│    └─Linear: 2-2                                 [10, 28]                  100,380\n",
      "├─ConvAutoencoder1D: 1-2                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-3                             [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-5                            [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-6                              [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-7                            [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-8                              [10, 128, 28]             --\n",
      "│    └─Linear: 2-4                                 [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-3                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-5                             [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-9                            [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-10                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-11                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-12                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-6                                 [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-4                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-7                             [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-13                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-14                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-15                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-16                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-8                                 [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-5                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-9                             [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-17                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-18                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-19                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-20                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-10                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-6                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-11                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-21                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-22                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-23                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-24                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-12                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-7                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-13                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-25                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-26                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-27                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-28                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-14                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-8                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-15                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-29                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-30                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-31                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-32                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-16                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-9                           [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-17                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-33                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-34                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-35                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-36                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-18                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-10                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-19                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-37                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-38                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-39                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-40                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-20                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-11                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-21                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-41                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-42                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-43                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-44                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-22                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-12                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-23                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-45                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-46                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-47                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-48                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-24                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-13                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-25                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-49                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-50                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-51                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-52                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-26                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-14                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-27                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-53                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-54                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-55                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-56                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-28                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-15                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-29                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-57                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-58                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-59                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-60                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-30                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-16                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-31                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-61                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-62                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-63                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-64                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-32                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-17                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-33                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-65                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-66                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-67                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-68                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-34                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-18                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-35                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-69                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-70                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-71                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-72                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-36                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-19                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-37                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-73                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-74                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-75                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-76                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-38                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-20                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-39                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-77                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-78                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-79                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-80                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-40                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-21                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-41                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-81                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-82                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-83                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-84                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-42                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-22                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-43                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-85                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-86                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-87                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-88                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-44                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-23                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-45                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-89                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-90                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-91                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-92                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-46                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-24                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-47                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-93                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-94                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-95                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-96                             [10, 128, 28]             --\n",
      "│    └─Linear: 2-48                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-25                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-49                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-97                           [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-98                             [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-99                           [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-100                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-50                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-26                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-51                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-101                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-102                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-103                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-104                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-52                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-27                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-53                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-105                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-106                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-107                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-108                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-54                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-28                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-55                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-109                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-110                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-111                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-112                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-56                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-29                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-57                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-113                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-114                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-115                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-116                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-58                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-30                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-59                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-117                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-118                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-119                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-120                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-60                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-31                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-61                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-121                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-122                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-123                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-124                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-62                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-32                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-63                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-125                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-126                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-127                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-128                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-64                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-33                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-65                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-129                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-130                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-131                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-132                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-66                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-34                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-67                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-133                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-134                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-135                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-136                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-68                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-35                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-69                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-137                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-138                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-139                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-140                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-70                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-36                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-71                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-141                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-142                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-143                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-144                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-72                                [10, 28]                  (recursive)\n",
      "├─ConvAutoencoder1D: 1-37                          [10, 28]                  (recursive)\n",
      "│    └─Sequential: 2-73                            [10, 128, 28]             (recursive)\n",
      "│    │    └─Conv1d: 3-145                          [10, 64, 28]              (recursive)\n",
      "│    │    └─ReLU: 3-146                            [10, 64, 28]              --\n",
      "│    │    └─Conv1d: 3-147                          [10, 128, 28]             (recursive)\n",
      "│    │    └─ReLU: 3-148                            [10, 128, 28]             --\n",
      "│    └─Linear: 2-74                                [10, 28]                  (recursive)\n",
      "├─PositionalEncoding: 1-38                         [37, 10, 28]              --\n",
      "│    └─Dropout: 2-75                               [37, 10, 28]              --\n",
      "├─TransformerEncoder: 1-39                         [37, 10, 28]              --\n",
      "│    └─ModuleList: 2-76                            --                        --\n",
      "│    │    └─TransformerEncoderLayer: 3-149         [37, 10, 28]              120,124\n",
      "│    │    └─TransformerEncoderLayer: 3-150         [37, 10, 28]              120,124\n",
      "│    │    └─TransformerEncoderLayer: 3-151         [37, 10, 28]              120,124\n",
      "│    │    └─TransformerEncoderLayer: 3-152         [37, 10, 28]              120,124\n",
      "│    │    └─TransformerEncoderLayer: 3-153         [37, 10, 28]              120,124\n",
      "│    │    └─TransformerEncoderLayer: 3-154         [37, 10, 28]              120,124\n",
      "├─Linear: 1-40                                     [10, 2]                   58\n",
      "====================================================================================================\n",
      "Total params: 966,266\n",
      "Trainable params: 966,266\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 321.67\n",
      "====================================================================================================\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 53.86\n",
      "Params size (MB): 3.31\n",
      "Estimated Total Size (MB): 57.21\n",
      "====================================================================================================\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "MACs (G):  548.83192\n",
      "Params (M):  0.826654\n",
      "Running time for a forward pass: 0.017950057983398438 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import time\n",
    "from thop import profile\n",
    "import math\n",
    "\n",
    "# Assuming Config and TransformerPredictor are defined elsewhere in your code\n",
    "config = Config()\n",
    "model = TransformerPredictor(config)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if getattr(torch, 'has_mps', False) else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Correctly define the size as a tuple of integers\n",
    "x = torch.rand((10, 37, 28)).to(device)  # 3-D tensor\n",
    "z = torch.rand((10, 37)).to(device)      # 2-D tensor]\n",
    "\n",
    "# Call the summary function\n",
    "print(summary(model, input_size=[(10, 37, 28), (10, 37)])\n",
    ")\n",
    "# Profile the model to get MACs and parameters\n",
    "macs, params = profile(model, inputs=(x, z))\n",
    "print('MACs (G): ', macs / 1000**2)\n",
    "print('Params (M): ', params / 1000**2)\n",
    "\n",
    "# Measure the time taken for a forward pass\n",
    "start_time = time.time()\n",
    "output = model(x, z)  # Ensure the correct number of arguments are passed\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Running time for a forward pass: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
